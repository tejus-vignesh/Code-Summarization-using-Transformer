{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sequence2Sequence Transformer for Code Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up the Environment - Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkBkw6o4OYHX",
        "outputId": "ae584414-0601-4ecd-f0d4-70866cacacbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-10 10:54:28.426525: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-08-10 10:54:28.483643: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-10 10:54:29.508267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting portalocker==2.1\n",
            "  Downloading portalocker-2.1.0-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm # for spacy tokenizer\n",
        "!pip install portalocker==2.1 # dependency for the torch.text library to work on google colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq5UBI1zOkF5",
        "outputId": "61cedd41-d172-44a9-af32-b56513cdadd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV5q4zWGJBkh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import regex as re\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from typing import Iterable, List\n",
        "\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHAW5I00JUmD"
      },
      "outputs": [],
      "source": [
        "train_filename = \"train.jsonl\"\n",
        "valid_filename = \"valid.jsonl\"\n",
        "test_filename = \"test.jsonl\"\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/CodeSearchNet-Python/\" # change this to your path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9Mcd-c7J8pu"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path, file_name):\n",
        "    compiled = []\n",
        "    # Regular expression pattern to remove docstrings from Python functions: matches both \"\"\"[docstring]\"\"\" and '''[docstring]'''\n",
        "    docstring_pattern  = r'\"\"\"[\\s\\S]*?\"\"\"|\\'\\'\\'[\\s\\S]*?\\'\\'\\'' \n",
        "    with open(f\"{file_path}{file_name}\", 'r') as file:\n",
        "        for line in file:\n",
        "            # Load JSON data to convert the string into a dictionary\n",
        "            data = json.loads(line)\n",
        "            if len(data['code_tokens']) < 256 and len(data['docstring_tokens']) < 256:\n",
        "                # Use the re.sub function to replace the matched pattern with an empty string\n",
        "                cleaned_code_text = re.sub(docstring_pattern, '', data['code'].strip())        \n",
        "                compiled.append({'code': cleaned_code_text, 'docstring': data['docstring'].strip()})\n",
        "\n",
        "    code_corpus = [data['code'] for data in compiled]\n",
        "    docstrings_corpus = [data['docstring'] for data in compiled]\n",
        "    \n",
        "    return code_corpus, docstrings_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8M0-VwSJ9qZ"
      },
      "outputs": [],
      "source": [
        "train_code_corpus, train_docstrings_corpus = load_data(file_path, train_filename)\n",
        "valid_code_corpus, valid_docstrings_corpus = load_data(file_path, valid_filename)\n",
        "test_code_corpus, test_docstrings_corpus = load_data(file_path, test_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building Vocabulary from the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP_EEaSeOq90"
      },
      "outputs": [],
      "source": [
        "# Initialize placeholder lists\n",
        "source_data = 'code'\n",
        "target_data = 'summary'\n",
        "\n",
        "tokenizer = {}\n",
        "vectorizer = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3hEIM6gJBki"
      },
      "outputs": [],
      "source": [
        "# Initilizing tokenizer to split the text into tokens\n",
        "tokenizer[source_data] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "tokenizer[target_data] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# function to yield list of tokens using the spacy tokenizer\n",
        "def yield_tokens(data_iter, language):\n",
        "    data_index = {source_data: 0, target_data: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield tokenizer[language](data_sample[data_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "unk_idx, pad_idx, bos_idx, eos_idx = 0, 1, 2, 3\n",
        "\n",
        "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [source_data, target_data]:\n",
        "    # Training data Iterator\n",
        "    train_iter = iter(zip(train_code_corpus, train_docstrings_corpus))\n",
        "    # Creating torchtext's Vocab Object\n",
        "    vectorizer[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=5,\n",
        "                                                    specials=special_tokens,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "for ln in [source_data, target_data]:\n",
        "  vectorizer[ln].set_default_index(unk_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Printing the Vocabulary Size for Code and Summary Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-djcjMeI3Vl",
        "outputId": "022ed813-e12d-40a3-ecfc-abfce02a7ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PythonCode Vocab Size:  63637\n",
            "Docstring Vocab Size:  31465\n"
          ]
        }
      ],
      "source": [
        "print(\"PythonCode Vocab Size: \", len(vectorizer[source_data]))\n",
        "print(\"Docstring Vocab Size: \", len(vectorizer[target_data]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization and Vector Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX9H-F9jJCo2"
      },
      "outputs": [],
      "source": [
        "# function to help perform sequential transformation on text data\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids):\n",
        "    return torch.cat((torch.tensor([bos_idx]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([eos_idx])))\n",
        "\n",
        "# helper object to perform tokenization, numericalization and adding EOS/BOS tokens usiing sequential_transforms function\n",
        "text_transform = {}\n",
        "for ln in [source_data, target_data]:\n",
        "    text_transform[ln] = sequential_transforms(tokenizer[ln], #Tokenization\n",
        "                                               vectorizer[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# tokenization, numericalization and adding EOS/BOS tokens by calling the helper object text_transform\n",
        "def vectorize(data):\n",
        "    code_corpus_ids, docstring_corpus_ids = [], []\n",
        "    for code_data, docstring_data in data:\n",
        "        code_ids = text_transform[source_data](code_data.rstrip(\"\\n\"))\n",
        "        docstring_ids = text_transform[target_data](docstring_data.rstrip(\"\\n\"))\n",
        "        if len(code_ids) < 256 and len(docstring_ids) < 256:\n",
        "            code_corpus_ids.append(code_ids)\n",
        "            docstring_corpus_ids.append(docstring_ids)\n",
        "    return list(zip(code_corpus_ids, docstring_corpus_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Converting the test strings into vectors, and also deleting unnecessary variables to free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEs8oRLPTHmv"
      },
      "outputs": [],
      "source": [
        "train_corpus_vectors = vectorize(zip(train_code_corpus, train_docstrings_corpus))\n",
        "del train_code_corpus, train_docstrings_corpus\n",
        "validate_corpus_vectors = vectorize(zip(valid_code_corpus, valid_docstrings_corpus))\n",
        "del valid_code_corpus, valid_docstrings_corpus\n",
        "test_corpus_vectors = vectorize(zip(test_code_corpus, test_docstrings_corpus))\n",
        "# del test_code_corpus, test_docstrings_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Positional Encoding, Token Embedding, and the Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRama-yZJBkk"
      },
      "outputs": [],
      "source": [
        "# Positional Encoding to help the model learn the position of the tokens in the sequence\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Transformer Network Module - Encoder-Decoder Architecture with Multi-Head Attention\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to add a subsequent word mask that will prevent the model from looking into the future words when making predictions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dm1WDH2JBkl"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == pad_idx).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == pad_idx).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtCjZbT6JBkn"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltxABNcKJBkn"
      },
      "outputs": [],
      "source": [
        "load_model = True\n",
        "save_model = True\n",
        "model_filepath = \"/content/drive/MyDrive/model_saves/my_checkpoint.pth.tar\" # change this to your path\n",
        "model_checkpoint_interval = 1\n",
        "sample_summarize_interval = 1\n",
        "\n",
        "n_epochs = 100\n",
        "learning_rate = 0.0001 \n",
        "eps = 1e-9\n",
        "source_vocab_size = len(vectorizer[source_data])\n",
        "target_vocab_size = len(vectorizer[target_data])\n",
        "embed_size = 256\n",
        "n_attention_heads = 8\n",
        "ffn_dim = 32\n",
        "batch_size = 64 \n",
        "n_encoder_layers = 3\n",
        "n_decoder_layers = 3\n",
        "\n",
        "\n",
        "transformer = Seq2SeqTransformer(n_encoder_layers, n_decoder_layers, embed_size,\n",
        "                                 n_attention_heads, source_vocab_size, target_vocab_size, ffn_dim) # Initialize model\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Code to Perform Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sAsWpNm08Lt"
      },
      "outputs": [],
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(device)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to take input code and generate summaries in english.\n",
        "def summarize_code(model: torch.nn.Module, sample_code: str):\n",
        "    model.eval()\n",
        "    src = text_transform[source_data](sample_code).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=bos_idx).flatten()\n",
        "    return \" \".join(vectorizer[target_data].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Collation\n",
        "\n",
        "This function adds padding to the sequences in each batch to make them of equal length.\n",
        "\n",
        "This is performed batch wise to reduce the amount of padding required, and hence reduce the amount of computation required.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y1_yfcQJBko"
      },
      "source": [
        "Training and evaluation loop that will be called for each\n",
        "epoch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(src_sample)\n",
        "        tgt_batch.append(tgt_sample)\n",
        "    src_batch = pad_sequence(src_batch, padding_value=pad_idx)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=pad_idx)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#####  Functions to perform Training and Evaluation of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkwkb82OJBko"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = train_corpus_vectors\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in tqdm(train_dataloader):\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        # torch.cuda.empty_cache()\n",
        "        losses += loss.item()\n",
        "    mean_epoch_loss = losses / len(list(train_dataloader))\n",
        "\n",
        "    return mean_epoch_loss\n",
        "\n",
        "\n",
        "def evaluate_model(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    validate_iter = validate_corpus_vectors\n",
        "    validate_dataloader = DataLoader(validate_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in validate_dataloader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        # torch.cuda.empty_cache()\n",
        "        losses += loss.item()\n",
        "\n",
        "    mean_eval_loss = losses / len(list(validate_dataloader))\n",
        "    return mean_eval_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9lr6qxtJBko"
      },
      "source": [
        "Load Checkpoint, if available and continue training from there. Save for every epoch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5ySD4g1qJBko",
        "outputId": "b4e0857f-eb63-420c-d47a-977c79b9fb4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2980 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 2980/2980 [05:06<00:00,  9.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme from current theme . \n",
            "\n",
            "\n",
            "Epoch: 38, Train loss: 2.407, Validation loss: 3.551, Epoch time = 309.450s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme name from current theme . \n",
            "\n",
            "\n",
            "Epoch: 39, Train loss: 2.394, Validation loss: 3.532, Epoch time = 305.496s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:02<00:00,  9.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme name from current theme . \n",
            "\n",
            "\n",
            "Epoch: 40, Train loss: 2.382, Validation loss: 3.558, Epoch time = 305.136s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:02<00:00,  9.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get theme from theme using theme name . \n",
            "\n",
            "\n",
            "Epoch: 41, Train loss: 2.370, Validation loss: 3.569, Epoch time = 305.102s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:02<00:00,  9.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme name based on theme name . \n",
            "\n",
            "\n",
            "Epoch: 42, Train loss: 2.358, Validation loss: 3.577, Epoch time = 304.593s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Set current theme to use . \n",
            "\n",
            "\n",
            "Epoch: 43, Train loss: 2.348, Validation loss: 3.575, Epoch time = 305.479s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:02<00:00,  9.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme name from current theme . \n",
            "\n",
            "\n",
            "Epoch: 44, Train loss: 2.336, Validation loss: 3.576, Epoch time = 305.031s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:04<00:00,  9.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme from current theme . \n",
            "\n",
            "     Default override is used to create a new theme name . \n",
            "\n",
            "\n",
            "Epoch: 45, Train loss: 2.325, Validation loss: 3.572, Epoch time = 306.467s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:02<00:00,  9.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get theme from theme . \n",
            "\n",
            "\n",
            "Epoch: 46, Train loss: 2.315, Validation loss: 3.584, Epoch time = 305.101s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme name from current theme . \n",
            "\n",
            "     : param override : default theme name . Default is None . \n",
            "     : return : theme name . \n",
            "\n",
            "\n",
            "Epoch: 47, Train loss: 2.305, Validation loss: 3.595, Epoch time = 305.517s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme name from current theme . \n",
            "\n",
            "     If no theme is given , use the default theme name . \n",
            "\n",
            "\n",
            "Epoch: 48, Train loss: 2.295, Validation loss: 3.588, Epoch time = 305.621s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme from current theme . \n",
            "\n",
            "     Default is ' <unk> ' , which returns a new theme name . \n",
            "\n",
            "\n",
            "Epoch: 49, Train loss: 2.286, Validation loss: 3.612, Epoch time = 306.087s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme name from theme . \n",
            "\n",
            "     Default is ' <unk> ' , which returns theme name . \n",
            "\n",
            "\n",
            "Epoch: 50, Train loss: 2.277, Validation loss: 3.624, Epoch time = 305.823s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme from current theme \n",
            "\n",
            "     : param override : default theme name \n",
            "     : returns : new theme name \n",
            "\n",
            "\n",
            "Epoch: 51, Train loss: 2.268, Validation loss: 3.623, Epoch time = 306.101s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme from theme . \n",
            "\n",
            "     : param override : name of theme to use . Default is None . \n",
            "     : returns : new theme name . \n",
            "\n",
            "\n",
            "Epoch: 52, Train loss: 2.259, Validation loss: 3.631, Epoch time = 305.994s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get a theme from the current theme . \n",
            "\n",
            "     : param override : default theme to use . Default : None . \n",
            "     : returns : a new theme name . \n",
            "\n",
            "\n",
            "Epoch: 53, Train loss: 2.251, Validation loss: 3.602, Epoch time = 306.098s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme from current theme . \n",
            "\n",
            "     If no theme is given , use current theme . If no theme is given , return current theme . \n",
            "\n",
            "\n",
            "Epoch: 54, Train loss: 2.243, Validation loss: 3.631, Epoch time = 305.575s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get theme from theme . \n",
            "\n",
            "     Default is used to create theme from theme . \n",
            "\n",
            "     : param override : name of theme to use . Default is None . \n",
            "     : returns : current theme used to use to use theme . \n",
            "\n",
            "\n",
            "Epoch: 55, Train loss: 2.234, Validation loss: 3.646, Epoch time = 306.058s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get theme from theme . \n",
            "\n",
            "     Default theme is used to override theme in the current theme . \n",
            "\n",
            "     : param override : theme to use to use to use theme ( defaults to current theme ) \n",
            "     : returns : theme name \n",
            "\n",
            "\n",
            "Epoch: 56, Train loss: 2.226, Validation loss: 3.647, Epoch time = 305.853s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme from current theme . \n",
            "\n",
            "     If no theme is given , use the current theme name . \n",
            "\n",
            "\n",
            "Epoch: 57, Train loss: 2.217, Validation loss: 3.641, Epoch time = 305.244s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Set theme to use . If no theme is given , use the current theme name . \n",
            "\n",
            "     If no theme is given , use the current theme name . \n",
            "\n",
            "\n",
            "Epoch: 58, Train loss: 2.211, Validation loss: 3.682, Epoch time = 305.526s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme from current theme \n",
            "\n",
            "\n",
            "Epoch: 59, Train loss: 2.203, Validation loss: 3.666, Epoch time = 305.672s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get theme from theme . \n",
            "\n",
            "     If no theme is given , use the current theme name . \n",
            "\n",
            "\n",
            "Epoch: 60, Train loss: 2.195, Validation loss: 3.683, Epoch time = 306.113s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:02<00:00,  9.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get current theme name from theme . \n",
            "\n",
            "     If no theme is specified , use the current theme name . \n",
            "\n",
            "     : param override : name of theme to use . If no theme name is given , use current theme name . \n",
            "     : returns : current theme name . \n",
            "\n",
            "\n",
            "Epoch: 61, Train loss: 2.189, Validation loss: 3.681, Epoch time = 305.103s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get theme from current theme . \n",
            "\n",
            "     If no theme is given , use the current theme name . If no theme is given , \n",
            "     will be used . \n",
            "\n",
            "\n",
            "Epoch: 62, Train loss: 2.182, Validation loss: 3.682, Epoch time = 306.111s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:05<00:00,  9.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get a theme from the theme . If no theme is given , use the current theme . \n",
            "\n",
            "     Default is used . \n",
            "\n",
            "\n",
            "Epoch: 63, Train loss: 2.175, Validation loss: 3.684, Epoch time = 307.713s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2980/2980 [05:03<00:00,  9.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint Saved!\n",
            "\n",
            "Sample Summary Generation:\n",
            "\n",
            "Reference:  Returns theme name.\n",
            "\n",
            "    Checks in this order:\n",
            "    1. override\n",
            "    2. cookies\n",
            "    3. settings\n",
            "Generated:   Get a theme from the theme . \n",
            "\n",
            "     Default is ' <unk> ' , which returns the theme name . \n",
            "\n",
            "     If no theme is given , use the current theme name . \n",
            "\n",
            "\n",
            "Epoch: 64, Train loss: 2.168, Validation loss: 3.701, Epoch time = 305.534s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 621/2980 [01:03<04:01,  9.78it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4f52eb2ecc9c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mepoch_counter\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmean_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_epoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-f25d2c9c14c7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtgt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epoch_counter = 1\n",
        "train_loss_list = []\n",
        "eval_loss_list = []\n",
        "\n",
        "if load_model:\n",
        "      checkpoint = torch.load(model_filepath)\n",
        "      transformer.load_state_dict(checkpoint[\"state_dict\"])\n",
        "      optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "      epoch_counter = checkpoint[\"epoch_counter\"] + 1\n",
        "      train_loss_list = checkpoint[\"train_loss_list\"]\n",
        "      eval_loss_list = checkpoint[\"eval_loss_list\"]\n",
        "\n",
        "while epoch_counter <= n_epochs+1:\n",
        "    start_time = timer()\n",
        "    mean_epoch_loss = train_model(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    train_loss_list.append(mean_epoch_loss)\n",
        "\n",
        "    mean_eval_loss = evaluate_model(transformer)\n",
        "    eval_loss_list.append(mean_eval_loss)\n",
        "    if save_model and epoch_counter % model_checkpoint_interval==0:\n",
        "        checkpoint = {\n",
        "            \"state_dict\": transformer.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"epoch_counter\": epoch_counter,\n",
        "            \"train_loss_list\": train_loss_list,\n",
        "            \"eval_loss_list\": eval_loss_list\n",
        "        }\n",
        "        torch.save(checkpoint, model_filepath)\n",
        "        print(\"\\nCheckpoint Saved!\\n\")\n",
        "        if mean_eval_loss < min(eval_loss_list):\n",
        "            torch.save(checkpoint, f\"/content/drive/MyDrive/model_saves/best_model_eval_loss{mean_eval_loss:.3f}_epoch_{epoch_counter}_checkpoint.pth.tar\") # change this to your path\n",
        "            print(\"\\nBest model by far, Saved it!\\n\")\n",
        "    if epoch_counter % sample_summarize_interval == 0:\n",
        "        print(\"Sample Summary Generation:\\n\")\n",
        "        print(\"Reference: \", test_docstrings_corpus[456])\n",
        "        print(\"Generated: \", summarize_code(transformer, test_code_corpus[456]))\n",
        "        print(\"\\n\")\n",
        "\n",
        "    print((f\"Epoch: {epoch_counter}, Train loss: {mean_epoch_loss:.3f}, Validation loss: {mean_eval_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    epoch_counter+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Sample Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ldAWp7SR879",
        "outputId": "f3321cd4-9322-4af3-897f-d77c69d93f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Summary Generation:\n",
            "\n",
            "Input Code:  def get_permissions(self):\n",
            "        \n",
            "\n",
            "        permissions = ''\n",
            "        if self.groups.filter(name='Admin').exists() or self.is_superuser:\n",
            "            permissions = 'admin'\n",
            "\n",
            "        return permissions\n",
            "Reference:  Returns the user's permissions.\n",
            "\n",
            "\n",
            "Generated:   Returns permissions of the user . \n"
          ]
        }
      ],
      "source": [
        "index = 4081\n",
        "print(\"Sample Summary Generation:\\n\")\n",
        "print(\"Input Code: \", test_code_corpus[index])\n",
        "print(\"Reference: \", test_docstrings_corpus[index])\n",
        "print(\"\\n\")\n",
        "print(\"Generated: \", summarize_code(transformer, test_code_corpus[index]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKbinHiyfWuM",
        "outputId": "d8159a23-982d-4a91-e567-118b94b46d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.12-py3-none-any.whl (266 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/266.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m194.6/266.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1.3->pytorch-ignite) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1.3->pytorch-ignite) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.12\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-ignite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing libraries for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYbNq0ySfIx4"
      },
      "outputs": [],
      "source": [
        "from ignite.metrics import RougeL\n",
        "from ignite.metrics.nlp import Bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to calculate the BLEU and RougeL scores for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABoaN3C7gOXI"
      },
      "outputs": [],
      "source": [
        "# below is the code to calculate the BLEU and rouge score for the model using the test dataset\n",
        "def calculate_bleu_rougel(model: torch.nn.Module, data):\n",
        "    model.eval()\n",
        "    targets = []\n",
        "    outputs = []\n",
        "    for src_sample, tgt_sample in tqdm(data):\n",
        "        src = text_transform[source_data](src_sample).view(-1, 1)\n",
        "        num_tokens = src.shape[0]\n",
        "        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "        tgt_tokens = greedy_decode(\n",
        "            model,  src, src_mask, max_len=num_tokens + 5, start_symbol=bos_idx).flatten()\n",
        "\n",
        "        generated_summary = vectorizer[target_data].lookup_tokens(list(tgt_tokens.cpu().numpy()))\n",
        "        correct_summary_tokens = tokenizer[target_data](tgt_sample.rstrip(\"\\n\"))\n",
        "\n",
        "        targets.append(correct_summary_tokens)\n",
        "        outputs.append(generated_summary)\n",
        "\n",
        "    rouge_metric = RougeL(multiref=\"best\")\n",
        "    bleu = Bleu(ngram=1, smooth=\"smooth1\")\n",
        "\n",
        "    bleu.update(([generated_summary], [correct_summary_tokens]))\n",
        "    rouge_metric.update((outputs, targets))\n",
        "\n",
        "    print(\"BLEU-1: \", bleu.compute())\n",
        "    print(\"RougeL: \", rouge_metric.compute())\n",
        "\n",
        "    return bleu.compute(), rouge_metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the BLEU and RougeL scores for the model on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW34FUoLlhiF",
        "outputId": "b60671de-6848-44dc-c597-5e30c4f0080f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11450/11450 [28:12<00:00,  6.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU-1:  tensor(0.1000, dtype=torch.float64)\n",
            "RougeL:  {'Rouge-L-P': 0.04868377655271807, 'Rouge-L-R': 0.766002440190948, 'Rouge-L-F': 0.766002440190948}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor(0.1000, dtype=torch.float64),\n",
              " {'Rouge-L-P': 0.04868377655271807,\n",
              "  'Rouge-L-R': 0.766002440190948,\n",
              "  'Rouge-L-F': 0.766002440190948})"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_bleu_rougel(transformer, list(zip(test_code_corpus,test_docstrings_corpus)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
